import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List

class ConvNeXtWithCSAttention(nn.Module):
    """å¸¦é€šé“+ç©ºé—´æ³¨æ„åŠ›çš„ConvNeXtç½‘ç»œ
    
    é›†æˆCBAMé£æ ¼çš„é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›æœºåˆ¶
    - Channel Attention: å­¦ä¹ ç‰¹å¾é€šé“é‡è¦æ€§
    - Spatial Attention: å­¦ä¹ ç©ºé—´ä½ç½®é‡è¦æ€§
    """
    
    def __init__(self, 
                 in_chans=3,
                 num_classes=1000, 
                 depths=[3, 3, 9, 3],
                 dims=[96, 192, 384, 768],
                 drop_path_rate=0.,
                 layer_scale_init_value=1e-6,
                 attention_stages=[False, True, True, True],  # å“ªäº›stageä½¿ç”¨æ³¨æ„åŠ›
                 ca_reduction=16,  # é€šé“æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹
                 sa_kernel_size=7):  # ç©ºé—´æ³¨æ„åŠ›å·ç§¯æ ¸å¤§å°
        """
        Args:
            attention_stages: æ¯ä¸ªstageæ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
            ca_reduction: é€šé“æ³¨æ„åŠ›ä¸­é—´å±‚é™ç»´æ¯”ä¾‹
            sa_kernel_size: ç©ºé—´æ³¨æ„åŠ›å·ç§¯æ ¸å¤§å°
        """
        super().__init__()
        
        # Stem Layer
        self.downsample_layers = nn.ModuleList()
        stem = nn.Sequential(
            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),
            LayerNorm(dims[0], eps=1e-6, data_format="channels_first")
        )
        self.downsample_layers.append(stem)
        
        # æ„å»ºä¸‹é‡‡æ ·å±‚
        for i in range(3):
            downsample_layer = nn.Sequential(
                LayerNorm(dims[i], eps=1e-6, data_format="channels_first"),
                nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),
            )
            self.downsample_layers.append(downsample_layer)

        # æ„å»ºå¸¦æ³¨æ„åŠ›çš„Stage
        self.stages = nn.ModuleList()
        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        
        cur = 0
        for i in range(4):
            stage_blocks = []
            for j in range(depths[i]):
                # å†³å®šæ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›
                use_attention = attention_stages[i] if i < len(attention_stages) else False
                
                if use_attention:
                    block = ConvNeXtBlockWithCSAttention(
                        dim=dims[i], 
                        drop_path=dp_rates[cur + j],
                        layer_scale_init_value=layer_scale_init_value,
                        ca_reduction=ca_reduction,
                        sa_kernel_size=sa_kernel_size
                    )
                else:
                    block = ConvNeXtBlock(
                        dim=dims[i], 
                        drop_path=dp_rates[cur + j],
                        layer_scale_init_value=layer_scale_init_value
                    )
                stage_blocks.append(block)
            
            stage = nn.Sequential(*stage_blocks)
            self.stages.append(stage)
            cur += depths[i]

        # åˆ†ç±»å¤´
        self.norm = LayerNorm(dims[-1], eps=1e-6, data_format="channels_first")
        self.head = nn.Linear(dims[-1], num_classes)
        self.apply(self._init_weights)
        
        # æ‰“å°æ³¨æ„åŠ›é…ç½®ä¿¡æ¯
        self._print_attention_info(attention_stages, dims)
        
    def _init_weights(self, m):
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            nn.init.trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def _print_attention_info(self, attention_stages, dims):
        """æ‰“å°æ³¨æ„åŠ›æœºåˆ¶é…ç½®ä¿¡æ¯"""
        print("ğŸ” æ³¨æ„åŠ›æœºåˆ¶é…ç½®:")
        for i, (use_att, dim) in enumerate(zip(attention_stages, dims)):
            status = "âœ… å¯ç”¨" if use_att else "âŒ å…³é—­"
            print(f"  Stage {i+1} (dim={dim:3d}): {status}")

    def forward_features(self, x):
        for i in range(4):
            x = self.downsample_layers[i](x)
            x = self.stages[i](x)
        return self.norm(x)

    def forward(self, x):
        x = self.forward_features(x)
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = torch.flatten(x, 1)
        x = self.head(x)
        return x


class ConvNeXtBlockWithCSAttention(nn.Module):
    """é›†æˆé€šé“+ç©ºé—´æ³¨æ„åŠ›çš„ConvNeXt Block
    
    æ³¨æ„åŠ›åº”ç”¨ä½ç½®ï¼šåœ¨æ·±åº¦å·ç§¯åï¼Œç‚¹å·ç§¯å‰
    è¿™æ ·å¯ä»¥åœ¨ä¿æŒç©ºé—´ä¿¡æ¯çš„åŒæ—¶è¿›è¡Œç‰¹å¾å¢å¼º
    """
    
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, 
                 ca_reduction=16, sa_kernel_size=7):
        super().__init__()
        
        # åŸå§‹ConvNeXtç»„ä»¶
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
        
        # é€šé“+ç©ºé—´æ³¨æ„åŠ› (CBAMé£æ ¼)
        self.channel_attention = ChannelAttention(dim, reduction=ca_reduction)
        self.spatial_attention = SpatialAttention(kernel_size=sa_kernel_size)
        
        # åç»­ç»„ä»¶
        self.norm = LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, 4 * dim)
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        
        # Layer Scale
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), 
                                requires_grad=True) if layer_scale_init_value > 0 else None
        
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        input = x
        
        # 1. æ·±åº¦å·ç§¯
        x = self.dwconv(x)
        
        # 2. åº”ç”¨é€šé“æ³¨æ„åŠ›
        x = self.channel_attention(x)
        
        # 3. åº”ç”¨ç©ºé—´æ³¨æ„åŠ›
        x = self.spatial_attention(x)
        
        # 4. åŸå§‹ConvNeXtçš„åç»­å¤„ç†
        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        
        if self.gamma is not None:
            x = self.gamma * x
            
        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)
        x = input + self.drop_path(x)
        return x


class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶ (CBAMé£æ ¼)
    
    é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–è·å–é€šé“ç»Ÿè®¡ä¿¡æ¯ï¼Œ
    ç„¶åé€šè¿‡MLPå­¦ä¹ é€šé“æƒé‡
    """
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        # å…±äº«çš„MLP
        self.mlp = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # åˆ†åˆ«è®¡ç®—å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–çš„é€šé“æ³¨æ„åŠ›
        avg_out = self.mlp(self.avg_pool(x))
        max_out = self.mlp(self.max_pool(x))
        
        # èåˆä¸¤ç§æ± åŒ–ç»“æœ
        out = avg_out + max_out
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        return x * self.sigmoid(out)


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ (CBAMé£æ ¼)
    
    é€šè¿‡é€šé“ç»´åº¦çš„å¹³å‡å’Œæœ€å¤§æ“ä½œè·å–ç©ºé—´ç»Ÿè®¡ä¿¡æ¯ï¼Œ
    ç„¶åé€šè¿‡å·ç§¯å­¦ä¹ ç©ºé—´æƒé‡
    """
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # åœ¨é€šé“ç»´åº¦è¿›è¡Œæ± åŒ–æ“ä½œ
        avg_out = torch.mean(x, dim=1, keepdim=True)  # [B, 1, H, W]
        max_out, _ = torch.max(x, dim=1, keepdim=True)  # [B, 1, H, W]
        
        # æ‹¼æ¥ä¸¤ç§æ± åŒ–ç»“æœ
        x_cat = torch.cat([avg_out, max_out], dim=1)  # [B, 2, H, W]
        
        # é€šè¿‡å·ç§¯å­¦ä¹ ç©ºé—´æ³¨æ„åŠ›
        x_out = self.conv(x_cat)  # [B, 1, H, W]
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        return x * self.sigmoid(x_out)


# åŸå§‹ConvNeXt Blockï¼ˆæ— æ³¨æ„åŠ›ï¼‰
class ConvNeXtBlock(nn.Module):
    """åŸå§‹ConvNeXt Block"""
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)
        self.norm = LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, 4 * dim)
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), 
                                requires_grad=True) if layer_scale_init_value > 0 else None
        
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        input = x
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        
        if self.gamma is not None:
            x = self.gamma * x
            
        x = x.permute(0, 3, 1, 2)
        x = input + self.drop_path(x)
        return x


class LayerNorm(nn.Module):
    """LayerNormå®ç°"""
    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        self.normalized_shape = (normalized_shape, )
    
    def forward(self, x):
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == "channels_first":
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x


def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
    if keep_prob > 0.0 and scale_by_keep:
        random_tensor.div_(keep_prob)
    return x * random_tensor


class DropPath(nn.Module):
    def __init__(self, drop_prob=None, scale_by_keep=True):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)


# æ¨¡å‹æ„å»ºå‡½æ•°
def convnext_tiny_cs_attention(attention_stages=[False, True, True, True], 
                              ca_reduction=16, sa_kernel_size=7, **kwargs):
    """ConvNeXt-Tiny with Channel+Spatial Attention
    
    Args:
        attention_stages: æ¯ä¸ªstageæ˜¯å¦å¯ç”¨æ³¨æ„åŠ› [Stage1, Stage2, Stage3, Stage4]
        ca_reduction: é€šé“æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹ï¼Œè¶Šå¤§è®¡ç®—é‡è¶Šå°
        sa_kernel_size: ç©ºé—´æ³¨æ„åŠ›å·ç§¯æ ¸å¤§å°ï¼Œé€šå¸¸ä½¿ç”¨7
    """
    model = ConvNeXtWithCSAttention(
        depths=[3, 3, 9, 3], 
        dims=[96, 192, 384, 768],
        attention_stages=attention_stages,
        ca_reduction=ca_reduction,
        sa_kernel_size=sa_kernel_size,
        **kwargs
    )
    return model


def convnext_small_cs_attention(attention_stages=[False, True, True, True], 
                               ca_reduction=16, sa_kernel_size=7, **kwargs):
    """ConvNeXt-Small with Channel+Spatial Attention"""
    model = ConvNeXtWithCSAttention(
        depths=[3, 3, 27, 3], 
        dims=[96, 192, 384, 768],
        attention_stages=attention_stages,
        ca_reduction=ca_reduction,
        sa_kernel_size=sa_kernel_size,
        **kwargs
    )
    return model


def convnext_base_cs_attention(attention_stages=[False, True, True, True], 
                              ca_reduction=16, sa_kernel_size=7, **kwargs):
    """ConvNeXt-Base with Channel+Spatial Attention"""
    model = ConvNeXtWithCSAttention(
        depths=[3, 3, 27, 3], 
        dims=[128, 256, 512, 1024],
        attention_stages=attention_stages,
        ca_reduction=ca_reduction,
        sa_kernel_size=sa_kernel_size,
        **kwargs
    )
    return model


# æµ‹è¯•å’Œå¯¹æ¯”ä»£ç 
if __name__ == "__main__":
    print("=== ConvNeXt + é€šé“ç©ºé—´æ³¨æ„åŠ› æµ‹è¯• ===")
    
    # æµ‹è¯•1: æ ‡å‡†é…ç½®
    print("\n1. æ ‡å‡†é…ç½®æµ‹è¯• (Stage2-4å¯ç”¨æ³¨æ„åŠ›):")
    model = convnext_tiny_cs_attention(
        attention_stages=[False, True, True, True],
        ca_reduction=16,
        sa_kernel_size=7
    )
    
    x = torch.randn(2, 3, 224, 224)
    with torch.no_grad():
        output = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    
    # æµ‹è¯•2: ä¸åŒé…ç½®ç­–ç•¥å¯¹æ¯”
    print(f"\n2. ä¸åŒæ³¨æ„åŠ›é…ç½®ç­–ç•¥å¯¹æ¯”:")
    
    configs = {
        'æ— æ³¨æ„åŠ›': [False, False, False, False],
        'ä»…Stage4': [False, False, False, True],
        'ä»…Stage3-4': [False, False, True, True],
        'æ ‡å‡†é…ç½®(Stage2-4)': [False, True, True, True],
        'å…¨éƒ¨å¯ç”¨': [True, True, True, True]
    }
    
    for name, att_stages in configs.items():
        if name == 'æ— æ³¨æ„åŠ›':
            # ä½¿ç”¨åŸå§‹ConvNeXtä½œä¸ºå¯¹æ¯”
            from ConvNeXtV1Model import convnext_tiny
            model_temp = convnext_tiny()
        else:
            model_temp = convnext_tiny_cs_attention(attention_stages=att_stages)
        
        params = sum(p.numel() for p in model_temp.parameters())
        
        # è®¡ç®—é¢å¤–å‚æ•°
        if name == 'æ— æ³¨æ„åŠ›':
            base_params = params
            extra_params = 0
        else:
            extra_params = params - base_params
        
        print(f"{name:15s}: {params:>10,} å‚æ•° (+{extra_params:>6,})")
    
    # æµ‹è¯•3: è¶…å‚æ•°å½±å“åˆ†æ
    print(f"\n3. è¶…å‚æ•°å¯¹å‚æ•°é‡çš„å½±å“:")
    
    print("é€šé“æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹å½±å“:")
    for reduction in [8, 16, 32]:
        model_temp = convnext_tiny_cs_attention(
            attention_stages=[False, False, True, True],
            ca_reduction=reduction
        )
        params = sum(p.numel() for p in model_temp.parameters())
        print(f"  reduction={reduction:2d}: {params:,} å‚æ•°")
    
    print("\nç©ºé—´æ³¨æ„åŠ›å·ç§¯æ ¸å¤§å°å½±å“:")
    for kernel_size in [3, 5, 7]:
        model_temp = convnext_tiny_cs_attention(
            attention_stages=[False, False, True, True],
            sa_kernel_size=kernel_size
        )
        params = sum(p.numel() for p in model_temp.parameters())
        print(f"  kernel_size={kernel_size}: {params:,} å‚æ•°")
    
    # æµ‹è¯•4: æ¨ç†é€Ÿåº¦æµ‹è¯•
    print(f"\n4. å‰å‘ä¼ æ’­æµ‹è¯•:")
    model.eval()
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(5):
            _ = model(x)
    
    # è®¡æ—¶
    import time
    times = []
    with torch.no_grad():
        for _ in range(10):
            start = time.time()
            _ = model(x)
            times.append(time.time() - start)
    
    avg_time = sum(times) / len(times)
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms (batch_size=2)")
    
    # æµ‹è¯•5: æ³¨æ„åŠ›æ¨¡å—ç»†èŠ‚
    print(f"\n5. æ³¨æ„åŠ›æ¨¡å—è¯¦ç»†ä¿¡æ¯:")
    
    # æŸ¥çœ‹ä¸€ä¸ªå¸¦æ³¨æ„åŠ›çš„block
    attention_block = None
    for stage in model.stages:
        for block in stage:
            if hasattr(block, 'channel_attention'):
                attention_block = block
                break
        if attention_block:
            break
    
    if attention_block:
        ca_params = sum(p.numel() for p in attention_block.channel_attention.parameters())
        sa_params = sum(p.numel() for p in attention_block.spatial_attention.parameters())
        
        print(f"å•ä¸ªBlockä¸­:")
        print(f"  é€šé“æ³¨æ„åŠ›å‚æ•°: {ca_params:,}")
        print(f"  ç©ºé—´æ³¨æ„åŠ›å‚æ•°: {sa_params:,}")
        print(f"  æ³¨æ„åŠ›æ€»å‚æ•°: {ca_params + sa_params:,}")
    
    print("\nâœ… ConvNeXt + é€šé“ç©ºé—´æ³¨æ„åŠ›æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("- æ¨èé…ç½®: Stage2-4å¯ç”¨æ³¨æ„åŠ›ï¼Œå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬")
    print("- è½»é‡é…ç½®: ä»…Stage3-4å¯ç”¨æ³¨æ„åŠ›ï¼Œå‡å°‘è®¡ç®—å¼€é”€")
    print("- é€šé“æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹16æ˜¯ç»éªŒæœ€ä¼˜å€¼")
    print("- ç©ºé—´æ³¨æ„åŠ›å·ç§¯æ ¸7Ã—7æ•ˆæœæœ€ä½³")
    print("- è¯¥é…ç½®ç›¸æ¯”åŸå§‹ConvNeXtä»…å¢åŠ å¾ˆå°‘å‚æ•°ï¼Œä½†æ€§èƒ½æå‡æ˜æ˜¾")